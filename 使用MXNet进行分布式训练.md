# 使用MXNet进行分布式训练

MXNet支持分布式训练，使我们能够利用多台机器进行更快速的训练。 在本文中，我们描述它是如何工作的，如何启动分布式训练工作和一些提供更多控制的环境变量。

##  并行类型

我们可以通过两种方式来将训练神经网络的工作量分配在多个设备（可以是GPU或CPU）。 第一种方式是数据并行，指的是每个设备存储完整模型副本的情况。 每个设备使用数据集的不同部分进行工作，设备共同更新共享模型。 这些设备可以位于一台机器上，也可以位于多台机器上。 在本文中，我们将描述如何以数据并行方式训练一个模型，其中设备分布在多台机器上。
当模型太大以至于不能适配设备内存时，第二种称为模型并行的方法就很有用。 在这里，不同的设备被分配了学习模型不同部分的任务。 目前，MXNet仅支持单机中的模型并行。 有关这方面的更多信息，请参考[使用模型并行的多GPU的培训](https://mxnet.incubator.apache.org/versions/master/faq/model_parallel_lstm.html)。

## 如何进行分布式训练

接下来的几个概念是理解使用MXNet进行分布式训练的关键：

### 进程的类型

MXNet中有三种进程类型，这些进程之间相互通信，完成模型的训练。

* Worker：Worker节点实际上在一批训练样本上进行训练。 在处理每个批次之前，Workers从服务器上拉出权重。 Worker还会在每批次处理后向服务器发送梯度(gradient)。 根据训练模型的工作量，在同一台机器上运行多个工作进程可能不是一个好主意。

* Server：可以有多个Servers存储模型的参数，并与Workers进行交流。 Serverd可能与工作进程同处一处,也可能不。

* Scheduler(调度器)：只有一个Scheduler。Scheduler的作用是配置集群。这包括等待每个节点启动以及节点正在监听哪个端口之类的消息。 然后Scheduler让所有进程知道集群中的其他节点的信息，以便它们可以相互通信。

### K-V 存储

MXNet提供了key-value存储机制，这个机制是多设备训练中关键的一部分。一个或多个Server通过将参数存储为K-V的形式在单台机器或者多台机器上进行跨节点的参数交互。这种存储机制中的每个值都由key-value表示，其中网络中的每个参数数组被分配了一个key,并且value是这个参数数组的权重。Workes在一批计算处理之后进行梯度的推送，并且在新的计算批次开始之前拉取更新后的权重。我们也可以在更新每个权重时传入K-V存储的优化器。这个优化器像随机梯度下降一样定义了一个更新规则——本质上是旧的权重、梯度和一些参数来计算新的权重。

如果你使用一个Gluon Trainer对象或者是模型的API,它将在内部使用kvstore对象来聚合梯度，这些梯度来自同一台机器上或者不同机器上多个设备上。

尽管无论是否使用多台机器进行训练，API都保持不变，但kvstore服务器的概念仅存在于分布式训练期间。在分布式情况下，每次推送和拉取都涉及与kvstore服务器的通信。当一台机器上有多个设备时，这些设备训练的梯度首先会聚合在机器上，然后发送再到服务器。
请注意，我们需要构建标志`USE_DIST_KVSTORE = 1`之后再编译MXNet才能使用分布式训练机制。

通过调用`mxnet.kvstore.create`函数使用包含dist字符串的字符串参数来启用KVStore的分布式模式，如下所示：

> kv = mxnet.kvstore.create('dist_sync')

有关KVStore的更多信息，请参阅[KVStore API](https://mxnet.incubator.apache.org/versions/master/api/python/kvstore/kvstore.html)。
